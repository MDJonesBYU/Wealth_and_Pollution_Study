{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd11b81",
   "metadata": {},
   "source": [
    "# Data Cleaning and Manipulation\n",
    "---\n",
    "## Purpose:\n",
    "Now that we have a cursory level of understanding of the data, we want to graphically understand how the data is distributed. To do that we need to establish some functions to clean and organize the data. This codebook contains functions to clean the data and organize it, so we can conduct meaningful exploratory data analysis (EDA). \n",
    "\n",
    "### Package Installation and Versioning Requirments:\n",
    "For questions regarding python version, package installations, and other functional requirements, see the *Read Me* file contained [here](link).\n",
    "\n",
    "Now, let's begin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e340c",
   "metadata": {},
   "source": [
    "### Import Packages and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36289f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Basic_data_structure_observations\n",
    "import numpy as np\n",
    "df_emissions, df_USDA, df_Redfin = Basic_data_structure_observations.load_base_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb088c4d",
   "metadata": {},
   "source": [
    "### Basic Data Cleaning Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787ca1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a couple things we want to consider with the data cleaning. Ultimately we want: \n",
    "#    1. Flexibility so we can use the function(s) for univariate and multivariate analysis\n",
    "#    2. Capable of cleaning for merged and non-merged data\n",
    "#    3. Efficient considering the significant record counts.\n",
    "\n",
    "# To do this, we're going to make a few functions that allow us to group the data by county and state levels\n",
    "# and we're also going to allow for aggregation so we can see total emissions by region, or sector specific based on\n",
    "# user input. \n",
    "\n",
    "# let's go ahead and do this. To start we need to have some weighted avg. functions to enable us to capture a closer \n",
    "# estimate of income. \n",
    "\n",
    "\n",
    "def weighted_avg(df,series):\n",
    "    \"\"\"Generic function for calculating weighted averages based on the population. This function works for groupby\n",
    "    applications, where the variable that you want to take the wegithed average of is listed in the .agg({dictionary}) \n",
    "    where you would put the weighted_avg function in the value segment. The USDA labor force column must exist in\n",
    "    the dataframe.\"\"\"\n",
    "    \n",
    "    return np.average(series, weights=df.loc[series.index, 'Civilian_labor_force_2021'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b57217",
   "metadata": {},
   "source": [
    "### Redfin Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd9a25aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period_begin</th>\n",
       "      <th>period_end</th>\n",
       "      <th>period_duration</th>\n",
       "      <th>region_type</th>\n",
       "      <th>region_type_id</th>\n",
       "      <th>table_id</th>\n",
       "      <th>is_seasonally_adjusted</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>...</th>\n",
       "      <th>sold_above_list_yoy</th>\n",
       "      <th>price_drops</th>\n",
       "      <th>price_drops_mom</th>\n",
       "      <th>price_drops_yoy</th>\n",
       "      <th>off_market_in_two_weeks</th>\n",
       "      <th>off_market_in_two_weeks_mom</th>\n",
       "      <th>off_market_in_two_weeks_yoy</th>\n",
       "      <th>parent_metro_region</th>\n",
       "      <th>parent_metro_region_metro_code</th>\n",
       "      <th>last_updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4/1/2017</td>\n",
       "      <td>4/30/2017</td>\n",
       "      <td>30</td>\n",
       "      <td>county</td>\n",
       "      <td>5</td>\n",
       "      <td>170</td>\n",
       "      <td>f</td>\n",
       "      <td>Morgan County, AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069595</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.001225</td>\n",
       "      <td>Decatur, AL</td>\n",
       "      <td>19460.0</td>\n",
       "      <td>1/9/2022 14:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/1/2014</td>\n",
       "      <td>12/31/2014</td>\n",
       "      <td>30</td>\n",
       "      <td>county</td>\n",
       "      <td>5</td>\n",
       "      <td>1457</td>\n",
       "      <td>f</td>\n",
       "      <td>Hennepin County, MN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098136</td>\n",
       "      <td>0.097744</td>\n",
       "      <td>-0.121909</td>\n",
       "      <td>-0.19722</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>-0.033469</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>33460.0</td>\n",
       "      <td>1/9/2022 14:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  period_begin  period_end  period_duration region_type  region_type_id  \\\n",
       "0     4/1/2017   4/30/2017               30      county               5   \n",
       "1    12/1/2014  12/31/2014               30      county               5   \n",
       "\n",
       "   table_id is_seasonally_adjusted               region  city      state  ...  \\\n",
       "0       170                      f    Morgan County, AL   NaN    Alabama  ...   \n",
       "1      1457                      f  Hennepin County, MN   NaN  Minnesota  ...   \n",
       "\n",
       "  sold_above_list_yoy price_drops  price_drops_mom  price_drops_yoy  \\\n",
       "0           -0.069595         NaN              NaN              NaN   \n",
       "1           -0.098136    0.097744        -0.121909         -0.19722   \n",
       "\n",
       "   off_market_in_two_weeks  off_market_in_two_weeks_mom  \\\n",
       "0                 0.020833                     0.020833   \n",
       "1                 0.172414                     0.034483   \n",
       "\n",
       "   off_market_in_two_weeks_yoy  parent_metro_region  \\\n",
       "0                     0.001225          Decatur, AL   \n",
       "1                    -0.033469      Minneapolis, MN   \n",
       "\n",
       "   parent_metro_region_metro_code    last_updated  \n",
       "0                         19460.0  1/9/2022 14:29  \n",
       "1                         33460.0  1/9/2022 14:29  \n",
       "\n",
       "[2 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Redfin.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b65f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>median_sale_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>46.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>281792.108696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>119265.458468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>158000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>192106.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>244500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>325547.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>715000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       median_sale_price\n",
       "count          46.000000\n",
       "mean       281792.108696\n",
       "std        119265.458468\n",
       "min        158000.000000\n",
       "25%        192106.250000\n",
       "50%        244500.000000\n",
       "75%        325547.500000\n",
       "max        715000.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll start with our Redfin data. Since we only want 2021 data, we're going to apply transformations to \n",
    "# convert the period-based data to datetime entires, and then filter to just sales identified in 2021. To\n",
    "# do that, we'll slice the dataframe starting on Jan 1st of that year, and ending by Dec. 31st. It may be \n",
    "# interesting to explore real estate prices and fluctuations in other years but that's not in scope for \n",
    "# this project. We just want to understand houses prices in 2021 as they relate to \n",
    "# expected emissions. \n",
    "\n",
    "\n",
    "def Clean_Redfin(df, level=\"STATE\"):\n",
    "    \"\"\"this function cleans the Redfin dataset for our analysis, returning a dataframe with only the columns \n",
    "    that we're interested in for Wealth vs Pollution analysis. This function requires the Redfin dataframe \n",
    "    and an aggregation level (\"STATE\" or \"COUNTY\" as inputs.\"\"\"\n",
    "\n",
    "    # Convert period data to timeseries object so we can pull from 2021 only. \n",
    "    df[['period_begin', 'period_end']] = df[['period_begin', 'period_end']].apply(pd.to_datetime)\n",
    "    \n",
    "    # Filter for sales data from 2021. We're also only going to consider residential properties. \n",
    "    df = df[(df[\"period_begin\"].dt.year==2021) & (df[\"property_type\"]==\"All Residential\") & \n",
    "            (df[\"region_type\"]==\"county\")].reset_index() \n",
    "    \n",
    "    # Remove nan sale entries: \n",
    "    df = df.dropna(subset=\"median_sale_price\")\n",
    "    \n",
    "    # Exclude DC. It's not a state.\n",
    "    df = df[df[\"state\"] != \"Columbia\"]     \n",
    "    \n",
    "    if level == \"STATE\":\n",
    "        # Take median sale prices at the state level\n",
    "        df = df.groupby(\"state\").agg(\n",
    "            {'median_sale_price': \"median\",\"state_code\": \"first\"}).reset_index()        \n",
    "\n",
    "    elif level == \"COUNTY\":\n",
    "        # Take median sale prices at the county level\n",
    "        df = df.groupby(\"region\").agg(\n",
    "            {'median_sale_price': \"median\", \"state\": \"first\", \"state_code\": \"first\"}).reset_index() \n",
    "    else: \n",
    "        raise ValueError(\"Level must be entered as 'STATE' or 'COUNTY'.\")\n",
    "        \n",
    "    # Next let's round the sale prices. Generally the prices have 5 significant digits, so we'll reduce broadly\n",
    "    # We're taking the log of the sale price (absolute value in case of quit-claim sales), rounding them to the\n",
    "    # nearest integer using floor and converting the float to an int. 5 is our sig digits, so N-int(...) \n",
    "    # gets us num digits to round by. \n",
    "    \n",
    "    # Then as a lambda function, we're rounding each element (x = row entry) by N-int digits of the sales column\n",
    "    N=5\n",
    "    df['median_sale_price'] = df['median_sale_price'].apply(lambda x: round(x, N - int(np.floor(np.log10(abs(x))))))\n",
    "\n",
    "    return(df)\n",
    "\n",
    "df_Redfin_Clean = Clean_Redfin(df_Redfin, \"STATE\")\n",
    "df_Redfin_Clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9edfe6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we see we have 46 states with sale price data, not too bad. And we can see the home prices skew\n",
    "# higher by about 30-35k. Not too bad. Let's now clean the USDA dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ba426",
   "metadata": {},
   "source": [
    "### USDA Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16dd2aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS_Code</th>\n",
       "      <th>State</th>\n",
       "      <th>Area_Name</th>\n",
       "      <th>Rural_Urban_Continuum_Code_2013</th>\n",
       "      <th>Urban_Influence_Code_2013</th>\n",
       "      <th>Metro_2013</th>\n",
       "      <th>Civilian_labor_force_2000</th>\n",
       "      <th>Employed_2000</th>\n",
       "      <th>Unemployed_2000</th>\n",
       "      <th>Unemployment_rate_2000</th>\n",
       "      <th>...</th>\n",
       "      <th>Civilian_labor_force_2021</th>\n",
       "      <th>Employed_2021</th>\n",
       "      <th>Unemployed_2021</th>\n",
       "      <th>Unemployment_rate_2021</th>\n",
       "      <th>Civilian_labor_force_2022</th>\n",
       "      <th>Employed_2022</th>\n",
       "      <th>Unemployed_2022</th>\n",
       "      <th>Unemployment_rate_2022</th>\n",
       "      <th>Median_Household_Income_2021</th>\n",
       "      <th>Med_HH_Income_Percent_of_State_Total_2021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142601576.0</td>\n",
       "      <td>136904853.0</td>\n",
       "      <td>5696723.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>162229903.0</td>\n",
       "      <td>153544980.0</td>\n",
       "      <td>8684923.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>164781642.0</td>\n",
       "      <td>158766998.0</td>\n",
       "      <td>6014644.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>69717.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2147173.0</td>\n",
       "      <td>2047731.0</td>\n",
       "      <td>99442.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>...</td>\n",
       "      <td>2259349.0</td>\n",
       "      <td>2183330.0</td>\n",
       "      <td>76019.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2286028.0</td>\n",
       "      <td>2226670.0</td>\n",
       "      <td>59358.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>53990.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FIPS_Code State      Area_Name  Rural_Urban_Continuum_Code_2013  \\\n",
       "0          0    US  United States                              NaN   \n",
       "1       1000    AL        Alabama                              NaN   \n",
       "\n",
       "   Urban_Influence_Code_2013  Metro_2013  Civilian_labor_force_2000  \\\n",
       "0                        NaN         NaN                142601576.0   \n",
       "1                        NaN         NaN                  2147173.0   \n",
       "\n",
       "   Employed_2000  Unemployed_2000  Unemployment_rate_2000  ...  \\\n",
       "0    136904853.0        5696723.0                     4.0  ...   \n",
       "1      2047731.0          99442.0                     4.6  ...   \n",
       "\n",
       "   Civilian_labor_force_2021  Employed_2021  Unemployed_2021  \\\n",
       "0                162229903.0    153544980.0        8684923.0   \n",
       "1                  2259349.0      2183330.0          76019.0   \n",
       "\n",
       "   Unemployment_rate_2021  Civilian_labor_force_2022  Employed_2022  \\\n",
       "0                     5.4                164781642.0    158766998.0   \n",
       "1                     3.4                  2286028.0      2226670.0   \n",
       "\n",
       "   Unemployed_2022  Unemployment_rate_2022  Median_Household_Income_2021  \\\n",
       "0        6014644.0                     3.7                       69717.0   \n",
       "1          59358.0                     2.6                       53990.0   \n",
       "\n",
       "   Med_HH_Income_Percent_of_State_Total_2021  \n",
       "0                                        NaN  \n",
       "1                                      100.0  \n",
       "\n",
       "[2 rows x 100 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_USDA.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72ea35d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Civilian_labor_force_2021</th>\n",
       "      <th>Median_Household_Income_2021</th>\n",
       "      <th>Unemployment_rate_2021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.474077e+06</td>\n",
       "      <td>69913.420842</td>\n",
       "      <td>4.796104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.182246e+06</td>\n",
       "      <td>11060.339138</td>\n",
       "      <td>1.235505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.783060e+05</td>\n",
       "      <td>50265.522681</td>\n",
       "      <td>2.613746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.859804e+06</td>\n",
       "      <td>63404.451677</td>\n",
       "      <td>3.837977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.221186e+06</td>\n",
       "      <td>67589.308793</td>\n",
       "      <td>4.759407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.719558e+06</td>\n",
       "      <td>78304.546044</td>\n",
       "      <td>5.569062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.794676e+07</td>\n",
       "      <td>91703.033007</td>\n",
       "      <td>7.300497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Civilian_labor_force_2021  Median_Household_Income_2021  \\\n",
       "count               5.000000e+01                     50.000000   \n",
       "mean                6.474077e+06                  69913.420842   \n",
       "std                 7.182246e+06                  11060.339138   \n",
       "min                 5.783060e+05                  50265.522681   \n",
       "25%                 1.859804e+06                  63404.451677   \n",
       "50%                 4.221186e+06                  67589.308793   \n",
       "75%                 7.719558e+06                  78304.546044   \n",
       "max                 3.794676e+07                  91703.033007   \n",
       "\n",
       "       Unemployment_rate_2021  \n",
       "count               50.000000  \n",
       "mean                 4.796104  \n",
       "std                  1.235505  \n",
       "min                  2.613746  \n",
       "25%                  3.837977  \n",
       "50%                  4.759407  \n",
       "75%                  5.569062  \n",
       "max                  7.300497  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Clean_USDA(df, level=\"STATE\"):\n",
    "    \"\"\"This function cleans the USDA dataset and performs aggregation  based on the level (COUNTY or STATE)\n",
    "    based on user declarations.\"\"\"\n",
    "    \n",
    "    # First, let's down-select to the columns we're interested in. Recall from the last notebook, that we \n",
    "    # will keep the 2013 rural/urban rankings as a coarse way to evaluate emission differences based on\n",
    "    # population density. \n",
    "    df = df[[\"State\", \"Area_Name\", 'Civilian_labor_force_2021', \"Rural_Urban_Continuum_Code_2013\",\n",
    "             \"Median_Household_Income_2021\", \"Unemployment_rate_2021\", \"FIPS_Code\",]].astype({\"FIPS_Code\": str})\n",
    "    \n",
    "    # We'll remove areas with nan for income, unemployment, or labor force. \n",
    "    df = df.dropna(subset =[\"Unemployment_rate_2021\", \"Median_Household_Income_2021\", \"Civilian_labor_force_2021\"])\n",
    "    \n",
    "    # To avoid misinterpretation of the unique FIPS code for each state and county, we'll convert it to string type.\n",
    "    # This will be important for merging our data later on with our emission data. The zfill ensures that each \n",
    "    # string is left padded with zeros to make each FIPS code entry 5 characters long (Ex. 00001, not 1 )\n",
    "    df[\"FIPS_Code\"] = df[\"FIPS_Code\"].astype(str).str.zfill(5)\n",
    "     \n",
    "    # Filter to the columns of interest. From this dataset we want the income, unemployment, labor force size, \n",
    "    # rural/urban designation, and the FIPs. Brining in Area_name is not necessary, but helpful for reading\n",
    "    # in our initial exploration\n",
    "    cols = [\"FIPS_Code\", \"Area_Name\", \"State\", \"Civilian_labor_force_2021\", \n",
    "            \"Median_Household_Income_2021\", \"Unemployment_rate_2021\", \"Rural_Urban_Continuum_Code_2013\"]\n",
    "    df = df[cols]\n",
    "    \n",
    "    # U.S. Territories are out of scope for this project, so we're removing them. The '~' is a negation operator\n",
    "    # so we get all the records where the states are not in the exclusion list. \n",
    "    exclusion_list = [\"PR\", \"DC\", \"US\"]\n",
    "    df = df[~df[\"State\"].isin(exclusion_list)].reset_index(drop=True)\n",
    "    \n",
    "    # We'll add a column for the state fips which will be useful later for aggregation at the state level\n",
    "    df[\"State_FIPS\"] = df['FIPS_Code'].apply(lambda x: x[:2])\n",
    "    \n",
    "    # Similar to the Redfin data, we're going to group this by level specification. Since we want to apply a \n",
    "    # weighted average based on population size. \n",
    "    if level==\"STATE\":\n",
    "        agg_dict = { \"FIPS_Code\": \"first\", \"Civilian_labor_force_2021\": \"sum\", \n",
    "                    \"Median_Household_Income_2021\": lambda x: weighted_avg(df, x),\n",
    "                    \"Unemployment_rate_2021\":lambda x: weighted_avg(df, x), \"State_FIPS\": \"first\"}\n",
    "\n",
    "        df = df.groupby(\"State\").agg(agg_dict).reset_index()\n",
    "        return(df)\n",
    "    \n",
    "    # Otherwise return county level (base)\n",
    "    else: \n",
    "        return(df)\n",
    "df_USDA_Clean = Clean_USDA(df_USDA, \"STATE\")\n",
    "df_USDA_Clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c8ecd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, we see that we have all 50 states accounted for in our dataset, and the numbers look reasonable. \n",
    "# The Median income, averaged across all 50 states is just shy of 70k, which is pretty close to estimates from\n",
    "# Census.Gov for 2021. You might notice that the labor force size average is skewed upwards due to large \n",
    "# populations in certain states. Meanwhile, the average unemployment rate across states falls pretty close\n",
    "# to the median. When we plot this it will be more apparent. \n",
    "\n",
    "#Let's now clean the Emission Data. This is going to be much more involved than the other datasources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32a5de6",
   "metadata": {},
   "source": [
    "### EPA Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca684232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE FIPS</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>SECTOR</th>\n",
       "      <th>COUNTY FIPS</th>\n",
       "      <th>POLLUTANT</th>\n",
       "      <th>POLLUTANT TYPE</th>\n",
       "      <th>EMISSIONS</th>\n",
       "      <th>UNIT OF MEASURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OR</td>\n",
       "      <td>41</td>\n",
       "      <td>Clackamas</td>\n",
       "      <td>Fires - Wildfires</td>\n",
       "      <td>005</td>\n",
       "      <td>Carbon Monoxide</td>\n",
       "      <td>CAP</td>\n",
       "      <td>1777086</td>\n",
       "      <td>TON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OR</td>\n",
       "      <td>41</td>\n",
       "      <td>Marion</td>\n",
       "      <td>Fires - Wildfires</td>\n",
       "      <td>047</td>\n",
       "      <td>Carbon Monoxide</td>\n",
       "      <td>CAP</td>\n",
       "      <td>1551703</td>\n",
       "      <td>TON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  STATE STATE FIPS     COUNTY             SECTOR COUNTY FIPS        POLLUTANT  \\\n",
       "0    OR         41  Clackamas  Fires - Wildfires         005  Carbon Monoxide   \n",
       "1    OR         41     Marion  Fires - Wildfires         047  Carbon Monoxide   \n",
       "\n",
       "  POLLUTANT TYPE  EMISSIONS UNIT OF MEASURE  \n",
       "0            CAP    1777086             TON  \n",
       "1            CAP    1551703             TON  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the EPA dataset has emissions by sectors, and we might want to probe sectoral as well as regional\n",
    "# differences. First, we're going to create some background functions to help us deal with condensing the\n",
    "# sectors into primary groups. Let's start with a function to allow us to condense the dataframe to \n",
    "# the primary emission sectors like (mobile, industrial, or commercial emissions)\n",
    "\n",
    "# Brief overview of data\n",
    "df_emissions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ca1cae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_emissions_by_major_sectors(df, level, options_dict):\n",
    "    \"\"\"This function takes in the emissions dataframe, a specified grouping level, and a nested dictionary of\n",
    "    primary and secondary emission sectors. The function flattens the nested dictionary, and returns a dataframe    \n",
    "    which has the total emissions separated by major sectors.\"\"\"\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    # To condense emissions to just primary keys, we'll call the K-V pairs using dict.items(). The \"sector\" is \n",
    "    # our key, and \"options\" is our value list. We'll be iterating using a for loop. \n",
    "    \n",
    "    for sector, options in options_dict.items():\n",
    "        # make a subset dataframe where the sectors are only those specified in the current K-V pair. \n",
    "        filtered_df = df[df['SECTOR'].isin(options)]\n",
    "        \n",
    "        # add the dataframe to a list, while adding a new column for our Primary sectors (the keys in our \n",
    "        # options dictionary)\n",
    "        df_list.append(filtered_df.assign(Major_Sector=sector))\n",
    "    \n",
    "    # Once done looping, concat the dataframes. \n",
    "    agg_df = pd.concat(df_list)\n",
    "    \n",
    "    # Next we'll use groupby method to group the emissions based on the major sector and the Count/State FIPS \n",
    "    # Depending on the level specified. \n",
    "    if level == \"COUNTY\": \n",
    "        grouped_df = agg_df.groupby([\"FIPS\", \"Major_Sector\"]).agg(\n",
    "            {'EMISSIONS': \"sum\", \"STATE\": \"first\", \"STATE FIPS\": \"first\", \"COUNTY\": \"first\", \n",
    "             \"COUNTY FIPS\":\"first\"}).reset_index()\n",
    "    elif level == \"STATE\":\n",
    "        grouped_df = agg_df.groupby([\"STATE FIPS\", \"Major_Sector\"]).agg(\n",
    "                    {'EMISSIONS': \"sum\", \"STATE\": \"first\"}).reset_index()\n",
    "        \n",
    "    # Basic error catch to see if the aggregation level was input wrong. \n",
    "    else: \n",
    "        raise ValueError(\"You must specify 'COUNTY' or 'STATE' for the level.\")\n",
    "        \n",
    "    # return output dataframe\n",
    "    return grouped_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dd87026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will make a function to generate the \"options_dict\" input to our above sector grouping function.\n",
    "\n",
    "\n",
    "def create_options_dict(input_set):\n",
    "    \"\"\"This function creates the options_dict that will be supplied when group_emissions_by_major_sector\n",
    "    is called. This function requires a flat dictionary listing all emission classification options. These\n",
    "    will be the secondary keys in our nested dictionary. \"\"\"\n",
    "\n",
    "    # There's 6 main categories we want, but there's not a broadly applicable way to do this. So, we're \n",
    "    # going to map those connections to the primary sector here.  \n",
    "    category_mapping = {\n",
    "        'Solvent': 'Industrial Processes',\n",
    "        'Gas Stations': 'Commercial',\n",
    "        'Agriculture': 'Industrial Processes',\n",
    "        'Commercial Cooking': 'Commercial',\n",
    "        'Bulk Gasoline Terminals': 'Industrial Processes',\n",
    "        'Waste Disposal': 'Industrial Processes',\n",
    "        'Dust': 'Industrial Processes',\n",
    "        'Biogenics': \"Miscellaneous Non-Industrial NEC\"\n",
    "    }\n",
    "\n",
    "    # Now we're going to create our new dictionary containing our primary sectors, and the secondary \n",
    "    # level(s) as a value list of string entries. \n",
    "    options_dict = {}\n",
    "    for option in input_set:\n",
    "        \n",
    "        # Identify Primary Sectors using the split method on ' - ', taking the first item from the \n",
    "        # resulting split list \n",
    "        category = option.split(' - ')[0]\n",
    "        \n",
    "        # Use .get method to see if our primary sector is in our category_mapping dictionary. We use\n",
    "        # category 2x to check both the keys and the values\n",
    "        mapped_category = category_mapping.get(category, category)\n",
    "        \n",
    "        # If the primary sector is not already in our new dictionary, we're going to add it. \n",
    "        if mapped_category not in options_dict:\n",
    "            options_dict[mapped_category] = []\n",
    "        \n",
    "        # Then, we're going to append the value for each element in (input_set) as a value \n",
    "        # corresponding to the appropriate primary sectory key (mapped category)\n",
    "        options_dict[mapped_category].append(option)\n",
    "        \n",
    "    # return output dictionary with primary and secondary keys\n",
    "    return(options_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7dd6a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we're ready to setup our cleaning function so that we can output a dataframe with flexibility \n",
    "# depending on the regional level, sector desired, and aggregation selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bbf0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean_EPA(df, level=\"STATE\", emission_contributor=\"residential\", agg=True):\n",
    "    \"\"\"This function cleans the EPA Emission dataset and performs aggregation  based on the level (COUNTY \n",
    "    or STATE), emission contributors (residential, industrial, etc), and sectoral aggregation. The function\n",
    "    returns a dataframe ready for analyzing emissions. emission_contributor options are 'residential', \n",
    "    'industrial', 'commercial','by sector', or 'all'. \"\"\"\n",
    "    \n",
    "    # Add a column for unique FIPDS ID if running at the county level.\n",
    "    if level==\"COUNTY\":\n",
    "        df[\"FIPS\"] = df[\"STATE FIPS\"].astype(str).str.cat(df[\"COUNTY FIPS\"].astype(str))\n",
    "    \n",
    "    # Filter based on emission_contributor specified.  \n",
    "    emission_sectors = set(df_emissions[\"SECTOR\"])\n",
    "    if emission_contributor == \"by sector\": \n",
    "        \n",
    "        # Get the options dictionary \n",
    "        options_dict = create_options_dict(emission_sectors)\n",
    "        \n",
    "        # Run the grouping function for the sectors\n",
    "        df = group_emissions_by_major_sectors(df, level, options_dict)\n",
    "        \n",
    "    elif emission_contributor in [\"residential\", \"industrial\", \"commercial\"]:\n",
    "        # Get dictionary of strings to search for each string entry in our sector list. \n",
    "        options_dict = {\n",
    "            \"residential\": [\"Residential\", \"Light Duty\"],\n",
    "            \"commercial\": [\"Comm/Institutional\", \"Commercial\", \"Dry Cleaning\", \"Graphic Arts\", \"Gas Stations\"],\n",
    "            \"industrial\": [\"Industrial\", \"Bulk\", \"Agriculture\", \"Waste\", \"Degreasing\", \"Electric Generation\", \n",
    "                           \"Locomotive\", \"Aircraft\"]}\n",
    "        \n",
    "        # Create a list of desired sectors via list comprehension. We do this by iterating through each entry in \n",
    "        # emission_sectors, and checking if any of the string entries from the value list corresponding to our \n",
    "        # sector key are in the list by using the .get method on our options dictionary.  \n",
    "        desired_sectors = [val for val in emission_sectors if any(option in val for option in options_dict.get(\n",
    "            emission_contributor, []))]\n",
    "        \n",
    "        # Reduce the dataframe to only consider the desired emissions sources \n",
    "        df = df[df[\"SECTOR\"].isin(desired_sectors)]\n",
    "        \n",
    "    elif emission_contributor != \"all\":\n",
    "        raise ValueError(\"\"\"You must specify one of the following options for sector:\\\n",
    "        'residential', 'industrial', 'commercial','by sector', or 'all' \"\"\")\n",
    "    \n",
    "    # Now group based on level and aggregation specified. \n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    if level == \"STATE\" and agg == True: \n",
    "        df = df.groupby(\"STATE FIPS\").agg(\n",
    "        {'EMISSIONS': \"sum\",\"STATE\": \"first\"}).reset_index() \n",
    "            \n",
    "    elif level == \"COUNTY\" and agg == True: \n",
    "        df = df.groupby(\"FIPS\").agg(\n",
    "        {'EMISSIONS': \"sum\",\"STATE FIPS\": \"first\", \"STATE\": \"first\", \n",
    "         \"COUNTY\": \"first\", \"COUNTY FIPS\": \"first\"}).reset_index()\n",
    "        \n",
    "    elif level == \"STATE\" and agg == False: \n",
    "        try: \n",
    "            df = df.groupby([\"Major_Sector\", \"STATE FIPS\"]).agg(\n",
    "                {'EMISSIONS': \"sum\", \"STATE\": \"first\"}).reset_index()         \n",
    "        except: \n",
    "            df = df.groupby([\"SECTOR\", \"STATE FIPS\"]).agg(\n",
    "                {'EMISSIONS': \"sum\", \"STATE\": \"first\"}).reset_index()        \n",
    "\n",
    "    # Prepare output dataframe\n",
    "    output_df = df.copy(deep=True)\n",
    "\n",
    "    if level==\"COUNTY\":\n",
    "        # Adding region tag to merge with Redfin data\n",
    "        output_df[\"Region\"] = output_df[\"COUNTY\"] + ' County, ' + output_df[\"STATE\"]\n",
    "    \n",
    "    # Remove non-states\n",
    "    exclusion_list = [\"TR\", \"DM\", \"PR\", \"VI\", \"DC\"]\n",
    "    output_df = output_df[~output_df[\"STATE\"].isin(exclusion_list)]\n",
    "\n",
    "    # Return the dataframe: \n",
    "    return(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23d22f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMISSIONS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3142.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5195.042648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9834.691370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1033.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2307.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5269.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>191813.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           EMISSIONS\n",
       "count    3142.000000\n",
       "mean     5195.042648\n",
       "std      9834.691370\n",
       "min         1.000000\n",
       "25%      1033.250000\n",
       "50%      2307.500000\n",
       "75%      5269.250000\n",
       "max    191813.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emissions_Clean = Clean_EPA(df_emissions, \"COUNTY\", \"residential\", agg=True)\n",
    "df_emissions_Clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a60e8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, we have established functions for cleaning our EPA, Redfin, and USDA datasets, and preparing them\n",
    "# for univariate analysis. We'll go ahead and finish our data cleaning and manipulation work by establishing a \n",
    "# function for merging the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a791a49",
   "metadata": {},
   "source": [
    "### Merging the Cleaned data together: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4f62953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merge_df (df_emission, df_USDA, df_Redfin, level):\n",
    "    \"\"\"This function will merge the data together into a singular dataframe based on the loaded dataframes from\n",
    "    the EPA, the USDA, and Redfin, based on the desired level of merging.\"\"\"\n",
    "    \n",
    "    \n",
    "    if level == \"STATE\": \n",
    "        # Provide list of merged columns we'll want\n",
    "        keep_cols = [\"STATE\", \"STATE FIPS\", \"EMISSIONS\", \"Civilian_labor_force_2021\", \n",
    "                         \"Median_Household_Income_2021\", \"Unemployment_rate_2021\", \"median_sale_price\",\n",
    "                        \"state\"]\n",
    "        \n",
    "        # Add major sector if needed\n",
    "        if \"Major_Sector\" in list(df_emission.columns): \n",
    "            keep_cols.append(\"Major_Sector\")\n",
    "        \n",
    "        # Merge the datasets at the state level\n",
    "        merged_df = pd.merge(df_emission, df_USDA, left_on='STATE FIPS', right_on=\"State_FIPS\", \n",
    "                             how='inner').reset_index()\n",
    "        merged_df = pd.merge(merged_df, df_Redfin, left_on='State', right_on=\"state_code\", \n",
    "                             how='inner').reset_index()\n",
    "        \n",
    "        # Reduce the dataframe to only desired columns \n",
    "        merged_df = merged_df[keep_cols]\n",
    "\n",
    "    elif level == \"COUNTY\":\n",
    "        \n",
    "        # To ensure approrpriate merging, we set the FIPS as string values \n",
    "        df_emission[\"FIPS\"] = df_emission[\"FIPS\"].astype(str)\n",
    "        df_USDA[\"FIPS_Code\"] = df_USDA[\"FIPS_Code\"].astype(str)\n",
    "        \n",
    "        # Provide the list of merged columns we'll want\n",
    "        keep_cols = [\"STATE\", \"STATE FIPS\", \"COUNTY FIPS\", \"COUNTY\", \"Rural_Urban_Continuum_Code_2013\", \n",
    "                     \"EMISSIONS\", \"Civilian_labor_force_2021\",\"Median_Household_Income_2021\", \n",
    "                     \"Unemployment_rate_2021\", \"median_sale_price\", \"FIPS\"]\n",
    "        \n",
    "        # Add major sector if doing sectoral analysis\n",
    "        if \"Major_Sector\" in df_emission.columns: \n",
    "            keep_cols.append(\"Major_Sector\")   \n",
    "        \n",
    "        # Merge dataframes together\n",
    "        merged_df = pd.merge(df_emission, df_USDA, left_on='FIPS', right_on=\"FIPS_Code\", \n",
    "                             how='inner').reset_index()\n",
    "\n",
    "        merged_df = pd.merge(merged_df, df_Redfin, left_on='Region', right_on=\"region\", how='inner').reset_index()\n",
    "        merged_df = merged_df[keep_cols]  \n",
    "\n",
    "    # Return the merged dataframe\n",
    "    return(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e03181ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the future we'll use the code blocks below for simple calls to get our county and state dataframes for analysis.  \n",
    "df_Redfin_County = Clean_Redfin(df_Redfin, \"COUNTY\")\n",
    "df_USDA_County = Clean_USDA(df_USDA, \"COUNTY\")\n",
    "df_emissions_County = Clean_EPA(df_emissions, \"COUNTY\", \"by sector\", agg=False)\n",
    "\n",
    "\n",
    "df_Redfin_State = Clean_Redfin(df_Redfin, \"STATE\")\n",
    "df_USDA_State = Clean_USDA(df_USDA, \"STATE\")\n",
    "df_emissions_State = Clean_EPA(df_emissions, \"STATE\", \"by sector\", agg=False)\n",
    "\n",
    "\n",
    "df_merged_state = get_merge_df(df_emissions_State,df_USDA_State,df_Redfin_State, level=\"STATE\")\n",
    "df_merged_cty = get_merge_df(df_emissions_County,df_USDA_County,df_Redfin_County, level=\"COUNTY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dacee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excellent, now we have functions that enable us to clean and filter the data with a lot of flexibility. If we want to \n",
    "# check pollution vs. wealth differences at state / county levels we can, and we can even drill down to whether we \n",
    "# want to include sectoral differences. \n",
    "\n",
    "# Obviously this flexibility comes at the cost of making a function which is less readible, and harder to immediately \n",
    "# understand. However, we chose this to maximize flexibility for future analysis so that only minor data manipulation \n",
    "# is needed down the road."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57db8ae9",
   "metadata": {},
   "source": [
    "### End of Notebook\n",
    "\n",
    "Next notebook: Univariate EDA \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276d920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
